## Q1. Using two jobs to finish the task. 

* Job1:
1. Mapper
Processes the input file(soc-LiveJournal1Adj.txt) and generates key-value pair.
2. Reducer
Get the common friend list of a pair of friends. A hashset is used.

* Job2:
Mapper:
Get the input pairs from the command line input.
Use these pairs as the condition to filter the list generated by Job1.

* Usage:
Input file: soc-LiveJournal1Adj.txt
Output file1: the full list of all friend pairs.
Output file2: the common friend list of pairs input from the command line.
cmd line to run hadoop task:
hadoop jar /local dir of jar file /*.jar /your hdfs dir/soc-LiveJournal1Adj.txt /your hdfs dir/out1 /your hdfs dir/out2 0,1#20,28193#1,29826#6#28041,28056#6222,19272

(0,1), (20,28193) and so on are input pairs for whom we generate the final filtered common friend list.

## Q2. Using two jobs to finish the task

* Job1:
Count the num of common friends for each pair. If the pair has no common friends they will not appear in the list.

* Job2:
Switch the key and value in the Mapper, sort records by the count in the decending order.
In the Reducer, swap the key and value and output top 10 records.

* Usage:
Input file: commonFriend.txt (generated by Q1 as output1)
Output file: the list of pair and common friend number. Only 10 records left.
cmd line to run the hadoop task:
hadoop jar /local dir of jar file /*.jar /your hdfs dir/commonFriend.txt /your hdfs dir/out

## Q3. Using in-memory join to find the inforation of common friends.
1. Mapper
In-memory join to find the info of each of the common friends. All pairs' records are generated.
2. Reducer
Using the input pair as filtering condition. Generate one record for the input pair.


* Usage:
Input file: commonFriend.txt (generated by Q1 as output1), userdata.txt
Output file: a pair, the information of the common friends. 
cmd line to run the hadoop task:
hadoop jar /local dir of jar file /*.jar commonFriend.txt  userdata.txt /your hdfs dir/out pair(e.g. 1,10)

## Q4. Using three jobs to finish the task

* Job1
Using in-memory join to generate the list: userID, oldest friend's age

* Job2
Sort the list from Job1 by its value.

* Job3
1. Mapper 1 processes the list from Job2, output top10 records.
2. Mapper 2 processes the userdata.txt, add tags and chooses first name and address for each user.
3. Reucer implements the reduce-side join to generate the final result.

* Usage:
Input files: soc-LiveJournal1Adj.txt userdata.txt
Output: temp1 is the output of Job1 and input to Job2, temp2 is the output of Job2 and input of Job3, out is the output of Job3.
hadoop jar /local dir of jar file /*.jar /your hdfs dir/soc-LiveJournal1Adj.txt /your hdfs dir/userdata.txt /your hdfs dir/temp2 /your hdfs dir/temp2 /your hdfs dir/out
